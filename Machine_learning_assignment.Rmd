---
output: word_document
---
# An alogrithm to identify how well you exercize 

##Executive summary: 
As part of the Coursera Machine Learning course this report  aims to identify the fedilety of how people exercize by using the data recorded from accelerometers on the belt, forearm, arm, and dumbell of an indivudal.

###Analysis data 
The data for this assignment was generated by 6 indivudals instructed to carry out the same exercize (dumbell curls) with diffrent levels of fedilitey to the ideal case as recorded in the classe variable; "A" being the best way to carry out the exercixe and "E" the worst. The corresponding acellerometer data during the test was recorded and stored in a number of additional potential predictor variables, more information can be found here [Data set details ](http://groupware.les.inf.puc-rio.br/har)

The data is used to develop a machine learning alogrithm using techniques learned in the course  that is capable of distingushing the classe of an exercize given the same predictors variables as defined in the training data set.The model is then used to  to predict 20 different test cases.
the training data can be found here 
(https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
and the test data here 
(https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

###read data set

```{r,echo=TRUE,results=TRUE,eval=TRUE,cache=TRUE}
rawtrainingset <- read.csv('pml-training.csv')
rawtestingset <- read.csv('pml-testing.csv')
```

###Preprocessing
Upon examination there are 159 variables in the dataset (see below). However many of these columns lack significant data or contain factordata that is notrelated to the sensor data. In particular columns 1:7 (Grey highlight) contain factor data, It is decided to exclude this for this first analyses whist it may effect the quality of the data, for example the "user" factor identifying the participant who carries out the exercize, one would expect each user to have a unique way of exercizing and this can contaminate signals from the acellerometers However it is reasonable to assumed this is a secondary effect since all the exercizes werecarried outunderthe supervision os a weight training expert. 

Put raw data SUMMARY table here

The preprocessing of the data was carried out using the **preprocessing()** script For brevity it is not diaplayed here but can be in this directory. To summarise the source function it removes the 1st 7 columns (non acellerometer data) and loops through all the other acellerometer columns to see if they contains more that 70% numeric content if the column has less numeric content than this it is discarded.

```{r,echo=TRUE,results=TRUE,eval=TRUE,cache=TRUE}
#===Load script to preprocess raw data
source("preprocessdat.R")

#process training set
trainingdata <- preprocesssdat(rawtrainingset)

names<-c(colnames(trainingdata))
names<-names[-1]# remove classe from list
names
#set testing set to have the same columns
testing <- rawtestingset[ ,names]
```

The preprocessing produces a reduced data set having only `r (length(names))` columns that have the best chance of being meaningful predictors for the classe factor.

### Data partition
The data was partitioned as follows the raw training set was split in two a training(75%) and validation(25%) subset 

```{r,echo=TRUE,eval=TRUE,cache=TRUE}
library(caret)
# create traing and validation subset in original training set 
trainset <- createDataPartition(y=trainingdata$classe,p=0.75, list=FALSE)
training <- trainingdata[trainset,]
validate <- trainingdata[-trainset,]

```


A principal component analysis using **Caret()**shows that 80% of the observedvariation can be captured in 12 components.
```{r,echo=TRUE,eval=TRUE,results=TRUE,cache=TRUE}
library(caret)
#Principal component analysis
preProc <- preProcess(training,method="pca",thresh=0.8)
train.PCs <- predict(preProc,training)
head(train.PCs,3)
```

As a 1st cut the PCA analysis was used to build a simple GLM regression model. Here a model is run on classe with principle components using **as.numeric()** to convert the classe factors to numbers, ie A:F becomes 1:5 .Beacuse the predicted outcomes is a real number the result is rounded to generate the final class prediction

```{r,echo=TRUE,eval=TRUE,results=TRUE,cache=TRUE}
preProc <- preProcess(training,method="pca",thresh=0.8)
train.PCs <- predict(preProc,training)
# GLM model outcome=classe and principle components use as.numeric to convert 
modelFit1 <- train(as.numeric(training$classe) ~.,method="glm",data=train.PCs)

# calculate principal component for validation data
validate.princ.comps <- predict(preProc,validate[,-1])

# used test data motel to predict validation data
validatepredictions<-predict(modelFit1,validate.princ.comps)

#..........note "validatepredictions" predictions are not integers 
validatepredictions<-round(predict(modelFit1,validate.princ.comps))

# compare results with confusion to get the accuracy
confusionMatrix(as.numeric(validate$classe),validatepredictions)
```


The model has only 26% accuracy each classe is ~1/5 of the data so we have a 20% chance of guessing right ie. this model is little better than chance.Its not clear why the PCA naalysis shows that 80 % o fthe varianceis captured by the 12 PC components but the model performs so badly, It is suspected to be the concersion of the class factor into a numeric variable.

Next a series of general ML alogrithms are tried to see which would perform best on the data 
  A Recursive partitioning tree model
  A gradient boosting machine model with 50 trees
  A gradient boosting machine model with 100 trees
  A random forest model with 50 trees.
Model code summarized below

```{r,echo=TRUE,eval=FALSE,results=FALSE,cache=TRUE}
#Recursive partitioning tree model
modelFit2<-train(classe ~.,method = "rpart",data = training)

#gradient boosting machine model
modelFit3 <- train(classe ~.,method="gbm",data=training,
                  verbose=FALSE,tuneGrid = gbmcontrol)

#Random forest model
modelFit4 <- train(classe ~.,method="rf",
                  data=training,prox=TRUE,ntree = 2)
```


###The  Recursive partitioning tree model
Clearly a partition style tree model is much better with an accuracy **~50%** The algorithm seems to struggle to identify classe "D" .To improve the predictions the next step is to look at aggregated type models .boosted tree models and random forests.. 

```{r,echo=FALSE,eval=TRUE,results=TRUE,cache=TRUE}
#A Recursive partitioning tree model
library(caret)
library(rpart.plot)
set.seed(125)

modelFit2<-train(classe ~.,method = "rpart",data = training)
  
#predictions for validation data
validatepredictions <- predict(modelFit2,newdata=validate[,-1])
#plot tree
rpart.plot(modelFit2$finalModel)
"Recursive tree model diagram"

# compare actual validation data compared to model prediction
"Recursive partitioning tree model results"
confusionMatrix(validate$classe,validatepredictions)

```


Clearly a partition style tree model does much better on this data **~50%** accuracy to improve the predictions the next step is to look at aggregated type models .boosted tree models and random forests.


####The gradient boosting machine model with 50 trees

HEre a gradient boosted model is run on the same data. The results are shown below  .his model has an impressive **93%** accuracy.The confusion matrix is plotted for the training data and the validation data.


```{r,echo=FALSE,eval=TRUE,results=TRUE,cache=TRUE}
#gradient boosting machine model with 50 trees
gbmcontrol <-  expand.grid(interaction.depth = c(1, 2, 4),
                           n.trees = 50,
                           shrinkage = 0.1,
                           n.minobsinnode = 3)

modelFit3a <- train(classe ~.,method="gbm",data=training,verbose=FALSE,tuneGrid = gbmcontrol)

#predictions for validation data
validatepredictions <- predict(modelFit3a,newdata=validate[,-1])

# compare actual validation data compared to model prediction
"gradient boosting machine model with 50 trees"
confusionMatrix(validate$classe,validatepredictions)

```

The model is rerun with n.trees=100 trees. An the results shown below now the prediction accuracy has increaced to **96% accuracy**.The confusion matrix is plotted for the training data and the validation data.

```{r,echo=FALSE,eval=TRUE,results=TRUE,cache=TRUE}
#gradient boosting machine model with 100 trees
gbmcontrol <-  expand.grid(interaction.depth = c(1, 2, 4),
                           n.trees = 100,
                           shrinkage = 0.1,
                           n.minobsinnode = 3)

modelFit3b <- train(classe ~.,method="gbm",data=training,verbose=FALSE,tuneGrid = gbmcontrol)

#predictions for validation data
validatepredictions <- predict(modelFit3b,newdata=validate[,-1])

# compare actual validation data compared to model prediction
"gradient boosting machine model with 100 trees"
confusionMatrix(validate$classe,validatepredictions)

```

####Random Forest model
The last model looked at was a random forest model. the results are shown below this model gas has~ **99% accuracy**. The confusion matrix is plotted for the training data and the validation data.

```{r,echo=FALSE,eval=TRUE,results=TRUE,cache=TRUE}
#Run a random forest model
modelFit4 <- train(classe ~.,method="rf",data=training,ntree=50)

# calculate .princ.compss for validation data
validatepredictions <- predict(modelFit4,newdata=validate)

# compare actual validation data compared to model prediction
" Random forest model , ntree=2"
confusionMatrix(validate$classe,validatepredictions)

```

To conclude the best model is the Random forest model here that model is used to predict the testing data set.

```{r,echo=TRUE,eval=TRUE,results=TRUE,cache=TRUE}

#Using model 4....random forest n=50 trees
testingpredictions <- predict(modelFit4,newdata=testing)

results<-c(1:length(testingpredictions))
  for (i in 1:length(testingpredictions)){#loop through all columns 
    results[i]<-paste(i,"=",testingpredictions[i],sep="")
  }
results
```




